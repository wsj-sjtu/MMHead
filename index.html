
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>

<link rel="stylesheet" type="text/css" href="style.css" />
  

<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>
<!-- <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'> -->
<!-- <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,300,300italic,400italic,600,600italic,700,700italic,800,800italic' rel='stylesheet' type='text/css'> -->
<link href='https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro' rel='stylesheet' type='text/css'>
  
<head>
    <title>MMHead: Towards Fine-grained Multi-modal 3D Facial Animation</title>
    <meta property="og:description" content="MMHead: Towards Fine-grained Multi-modal 3D Facial Animation"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6HHDEXF452"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-6HHDEXF452');
</script>

</head>


<body>
<div class="container">
    <div class="paper-title">
      <h1>MMHead: Towards Fine-grained Multi-modal 3D Facial Animation</h1>
    </div>
  

    
    <div id="authors">
        <div class="author-row">
            <div class="col-6 text-center">Sijing Wu<sup>1*</sup></div>
            <div class="col-6 text-center">Yunhao Li<sup>1*</sup></div>
            <div class="col-6 text-center"><a href="https://daodaofr.github.io/">Yichao Yan</a><sup>1</sup></div>
            <div class="col-6 text-center"><a href="https://duanhuiyu.github.io/">Huiyu Duan</a><sup>1</sup></div>
            <div class="col-6 text-center"><a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup>2</sup></div>
            <div class="col-6 text-center"><a href="https://scholar.google.com/citations?user=E6zbSYgAAAAJ&hl=en&oi=ao">Guangtao Zhai</a><sup>1</sup></div>
        </div>

        <div class="affil-row">
            <div class="col-2 text-center"><sup>1</sup><a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a></div>
            <div class="col-2 text-center"><sup>2</sup><a href="https://www.ntu.edu.sg/">Nanyang Technological University</a></div>
            <br>
        </div>

        <table align=center width="30%">
            <tr>
            <td colspan="1">
                <a href="" target="_blank">
                    &nbsp;&nbsp;&nbsp;&nbsp;<image src="assets/paper_icon.png" height="55px">
                    <h5><strong>Paper</strong></h5>
                </a>
            </td>
            <td colspan="1">
                <a href="https://github.com/wsj-sjtu/MMHead" target="_blank">
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<image src="assets/github_icon.png" height="55px">
                    <h5><strong>Code</strong></h5>
                </a>
            </td>
            <td colspan="1">
                <a href="https://www.youtube.com/watch?v=nnggJZhiEW4" target="_blank">
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<image src="assets/youtube_icon.png" height="55px">
                    <h5><strong>Video</strong></h5>
                </a>
            </td>
            </tr>
         </table>
      
    </div>

    <section id="teaser">
        <a href="assets/teaser.png">
            <img width="100%" src="assets/teaser.png">
        </a>
        <p align=center class="caption">We present MMHead, the first multi-modal 3D facial animation dataset with hierarchical text annotations including abstract descriptions for overall actions and emotions, and fine-grained descriptions for expressions, head poses, as well as possible scenarios that may cause such emotions.
        </p>
    </section>

    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr/>
        <p>
          3D facial animation has attracted considerable attention due to its extensive applications in the multimedia field.
Audio-driven 3D facial animation has been widely explored with promising results. However, multi-modal 3D facial animation, especially text-guided 3D facial animation is rarely explored due to the lack of multi-modal 3D facial animation dataset.
To fill this gap, we first construct a large-scale multi-modal 3D facial animation dataset, <strong>MMHead</strong>, which consists of 49 hours of 3D facial motion sequences, speech audios, and rich hierarchical text annotations. Each text annotation contains abstract action and emotion descriptions, fine-grained facial and head movements (i.e., expression and head pose) descriptions, and three possible scenarios that may cause such emotion.
Concretely, we integrate five public 2D portrait video datasets, and propose an automatic pipeline to 1) reconstruct 3D facial motion sequences from monocular videos; and 2) obtain hierarchical text annotations with the help of AU detection and ChatGPT. 
Based on the MMHead dataset, we establish benchmarks for two new tasks: text-induced 3D talking head animation and text-to-3D facial motion generation. 
Moreover, a simple but efficient VQ-VAE-based method named MM2Face is proposed to unify the multi-modal information and generate diverse and plausible 3D facial motions, which achieves competitive results on both benchmarks. 
Extensive experiments and comprehensive analysis demonstrate the significant potential of our dataset and benchmarks in promoting the development of multi-modal 3D facial animation.
        </p>
    </section>
    
    
<!--     <section id="Video">
        <h2>Demo Video</h2>
        <hr/>
        <figure style="width: 100%;">
            <video class="centered" width="100%" controls muted loop autoplay>
                <source src="assets/demo.mp4" type="video/mp4">
            </video>
            <p class="caption">The demo video shows some samples of the generated 3D facial motion and 2D singing portrait video according to the input singing audio.
            </p>
        </figure>
        <hr/>
    </section> -->
   
   


</div>
</body>
</html>
